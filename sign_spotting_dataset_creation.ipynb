{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a530ee",
   "metadata": {},
   "source": [
    "# Dataset creation for sign spotting\n",
    "Here, we preprocess the data from the Corpus NGT and split it up by its annotations. The data corresponding to an annotation are then sampled so they are of a fixed length. \n",
    "\n",
    "If the target fixed length is X frames, we can check if an annotation is already X frames long. If so, we can simply add its (preprocessed) data as a new entry in the train, validation or test set. If instead the length of the annotation is less than X frames, we add zero padding. \n",
    "\n",
    "If the annotation is longer than X frames, we undersample the frames. For instance, assuming we have a target length X=10 and an annotation with 22 frames, we first drop every other (every second) frame to get to 11 frames. We do not use random undersampling here to ensure at least some of the temporal structure of the annotation is maintained. Then, we drop 1 more frame randomly to get to our target of 10 frames.\n",
    "\n",
    "We first create the dataset using the above method, before repeating the 'dataset creation' for the purposes of checking the correlation between the used features. The repeat is needed because for the correlation analysis, we do not want to  undersampling and zero padding be done, we simply use the annotations at their original length. After preparing for the correlation analysis, we use masking of the NaN values to not skew the correlation (e.g. by replacing NaNs with zeros). Lastly, we find out which features have correlation above some threshold so we can discard them before training our sign spotting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e0d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pympi\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Keep python tools up to date\n",
    "from tools import tools, feature_extraction, constants, mediapipe_tools, make_dataset\n",
    "importlib.reload(mediapipe_tools)\n",
    "importlib.reload(tools)\n",
    "importlib.reload(constants)\n",
    "importlib.reload(feature_extraction)\n",
    "importlib.reload(make_dataset)\n",
    "\n",
    "# Import all functions from the tools\n",
    "from tools.tools import*\n",
    "from tools.mediapipe_tools import normalise_coordinates, get_pixel_coordinates\n",
    "from tools.constants import PATHS # Path constants\n",
    "from tools.feature_extraction import get_wrist_angle_distance\n",
    "from tools.make_dataset import make_dataset\n",
    "\n",
    "np.random.seed(123) # Set random seed for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9163a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CNGT files when split into numpy files: 3420\n"
     ]
    }
   ],
   "source": [
    "# We find all of the landmark files, split up by the different areas\n",
    "# E.g. 'face' has its own numpy file for each video\n",
    "path = PATHS['np_landmarks']\n",
    "cngt_lmrks = find_files(path, '.npy')\n",
    "print('Number of CNGT files when split into numpy files:', len(cngt_lmrks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6866b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n"
     ]
    }
   ],
   "source": [
    "# Root where all the annotated .eaf sign files are present\n",
    "dataset_root = PATHS['cngt_vids_and_eaf']\n",
    "\n",
    "# This is a path to a file of annotations tailored to the dataset creation\n",
    "# It only distinguishes between handedness, not manual simultaneity\n",
    "# Because we don't care about whether another sign is produced simultaneously\n",
    "# For making the dataset, we only want to make sure two-handed signs\n",
    "# Are added once as an annotation and not twice, so that's why we distinguish handedness\n",
    "dataset_anns_path = PATHS['dataset_anns']\n",
    "\n",
    "# List the .eaf files in the root directory to investigate\n",
    "anns_in_dir = [file for file in os.listdir(dataset_root) if file.endswith('.eaf')]\n",
    "\n",
    "# Loading the annotations for the dataset-tailored annotations\n",
    "# Or we create them if they don't exist yet\n",
    "if os.path.exists(dataset_anns_path):\n",
    "    print('Loading annotations...')\n",
    "    anns_with_tiers = load_dict(dataset_anns_path)\n",
    "else:\n",
    "    print('Making annotations without manual simultaneity...')\n",
    "    anns_with_tiers = {}\n",
    "    for i, ann_file in enumerate(anns_in_dir):\n",
    "        print(i, end = '\\r')\n",
    "        # Read in the Eaf file \n",
    "        eaf_file = pympi.Elan.Eaf(os.path.join(dataset_root, ann_file))\n",
    "\n",
    "        # Get the glosses and mouthings of the file\n",
    "        anns_dict, _ = get_gloss_vals(eaf_file, True)\n",
    "        # As explained above, we only distinguish handedness and not simultaneity\n",
    "        anns_dict = man_sim_and_hand_dist(anns_dict, manual_sim = False)\n",
    "\n",
    "        # Store the glosses, mouthings and tiers\n",
    "        anns_with_tiers[ann_file] = anns_dict\n",
    "    print('Storing...')\n",
    "    with open(dataset_anns_path, 'wb') as f:\n",
    "        pickle.dump(anns_with_tiers, f)\n",
    "        \n",
    "# Signbank dictionary info\n",
    "df = pd.read_csv(PATHS['signbank_with_linguistics'])\n",
    "\n",
    "# Dictionary which contains which videos (and signers) belong in test/train set\n",
    "id_split = load_dict(PATHS['CNGT_split_ids'])\n",
    "\n",
    "# Using only the top X signs if we have a list of them\n",
    "top = -1\n",
    "top_signs_path = PATHS['top_signs'].format(top)\n",
    "if os.path.exists(top_signs_path):\n",
    "    top_signs = np.load(top_signs_path)\n",
    "else:\n",
    "    top_signs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cf900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signer split across train, validation and test sets\n",
      "Train ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S013', 'S014', 'S015', 'S016', 'S017', 'S018', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S029', 'S030', 'S031', 'S032', 'S033', 'S034', 'S035', 'S036', 'S037', 'S038', 'S043', 'S044', 'S045', 'S049', 'S050', 'S059', 'S060', 'S061', 'S062', 'S063', 'S064', 'S065', 'S066', 'S067', 'S068', 'S069', 'S070', 'S075', 'S076', 'S077', 'S078', 'S083', 'S084', 'S087', 'S088', 'S089', 'S090', 'S091', 'S092']\n",
      "Val ['S011', 'S012', 'S019', 'S020', 'S027', 'S028', 'S039', 'S040', 'S041', 'S045', 'S046', 'S047', 'S048', 'S051', 'S052', 'S055', 'S056', 'S058', 'S071', 'S072', 'S073', 'S074', 'S079', 'S080', 'S081', 'S082', 'S085', 'S086']\n",
      "Test ['S011', 'S012', 'S019', 'S020', 'S027', 'S028', 'S039', 'S040', 'S041', 'S042', 'S045', 'S046', 'S047', 'S048', 'S051', 'S052', 'S053', 'S054', 'S055', 'S056', 'S057', 'S058', 'S071', 'S072', 'S073', 'S074', 'S080', 'S081', 'S082', 'S085', 'S086']\n"
     ]
    }
   ],
   "source": [
    "print('Signer split across train, validation and test sets')\n",
    "# Printing which signers belong in the train, validation and test set\n",
    "for key in id_split:\n",
    "    print(key, sorted(set([x.split('_')[-1] for x in id_split[key]])))\n",
    "# Note that the val and test sets are allowed to have the same signers (same Sxxx)\n",
    "# But that they should contain different videos\n",
    "val_vids = id_split['Val']\n",
    "test_vids = id_split['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97ee55",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature extraction\n",
    "**Note:** this codeblock can take a while to run for the first time due to the preprocessing and extraction for each video being computationally heavy. \n",
    "\n",
    "*Expected runtime: 15-30min*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8dea359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-extracted data exists, loading...\n",
      "Done!\n",
      "CPU times: total: 1.8 s\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Here we pick if we want to make a dataset with feature extraction (based on linguistics)\n",
    "# Or if we want to just extract the landmarks (done if only_features = False)\n",
    "only_features = True\n",
    "features = '' if only_features else '_only_lmrks'\n",
    "\n",
    "# Path where the extracted features are stored\n",
    "features_path = PATHS['features_data'].format(features)\n",
    "\n",
    "# If the extracted features are not saved yet, we create them and then store them\n",
    "if not os.path.exists(features_path):\n",
    "    print('Creating feature-extracted data...')\n",
    "    features_data = {}\n",
    "    for i, k in enumerate(anns_with_tiers):\n",
    "        print('Extracting features for video {}/{}'.format(i+1, len(list(anns_with_tiers.keys()))), end = '\\r')\n",
    "        # Get the glosses of the file\n",
    "        anns_dict = anns_with_tiers[k]\n",
    "    \n",
    "        # Loading the numpy files for a specific video\n",
    "        lmrk_dict = load_numpy(cngt_lmrks, k.replace('.eaf', ''))\n",
    "        # Normalize the coordinates, also create a horizontally flipped version\n",
    "        lmrk_dict = normalise_coordinates(get_pixel_coordinates(lmrk_dict))\n",
    "        pose = lmrk_dict['pose']\n",
    "        \n",
    "        if 'only_lmrks' in features_path:\n",
    "            l_lmrk = lmrk_dict['l_hand']\n",
    "            r_lmrk = lmrk_dict['r_hand']\n",
    "            shape = l_lmrk.shape\n",
    "            l_lmrk = l_lmrk.reshape(shape[0], -1)\n",
    "            r_lmrk = r_lmrk.reshape(shape[0], -1)\n",
    "        else:\n",
    "            # Getting landmarks preprocessed, with new format being (wrist, distances, angles) for each frame\n",
    "            l_lmrk = get_wrist_angle_distance(lmrk_dict, 'l_hand', pose).astype(np.float32)\n",
    "            r_lmrk = get_wrist_angle_distance(lmrk_dict, 'r_hand', pose).astype(np.float32)\n",
    "        # Store in the dictionary\n",
    "        features_data[k] = {'l_hand' : l_lmrk, 'r_hand': r_lmrk}\n",
    "    \n",
    "    # Store the extracted features in a numpy file (faster than pickling)\n",
    "    features_data_np = np.array(list(features_data.items()))\n",
    "    np.save(features_path, features_data_np)\n",
    "        \n",
    "else:\n",
    "    print('Feature-extracted data exists, loading...')\n",
    "    # Allow pickle has to be true because we have nested dictionaries\n",
    "    features_data = dict(np.load(features_path, allow_pickle = True))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10272186",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727732be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 33s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fixed_length = 10\n",
    "X, y, stats, var_len, ann_lengths, glosses = make_dataset(anns_with_tiers, features_data, df, val_vids, test_vids, \n",
    "                                                          top_signs, id_split, fixed_length=fixed_length, \n",
    "                                                          zero_pad=True, only_features = only_features)\n",
    "smaller, precise, larger = stats\n",
    "X_train_var_len, y_train_var_len = var_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ac14e",
   "metadata": {},
   "source": [
    "# Analyzing annotation length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056f304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Showing how long the annotations are (in frames)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title('Cumulative density plot of annotation length')\n",
    "plt.xlim(0,50)\n",
    "plt.hist(ann_lengths, bins = 250, cumulative = True, density = True)\n",
    "plt.xlabel('Length (in frames)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a736d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean annotation length 8.0\n",
      "Number of annotations, smaller than 10 frames, larger than 10 frames, exactly 10 frames\n",
      "110435 0.63 0.32 0.05\n"
     ]
    }
   ],
   "source": [
    "print('Mean annotation length', np.median(ann_lengths))\n",
    "\n",
    "total = smaller + larger + precise\n",
    "print('Number of annotations, smaller than {0} frames, larger than {0} frames, exactly {0} frames'.format(fixed_length))\n",
    "print(total, round(smaller/total, 2), round(larger/total, 2), round(precise/total, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec624d58",
   "metadata": {},
   "source": [
    "# Further processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaeb36b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique glosses: 2894\n"
     ]
    }
   ],
   "source": [
    "top_suffix = '_top' if len(top_signs) != 0 else ''\n",
    "\n",
    "# Get the list of possible glosses\n",
    "glosses = list(set(glosses))\n",
    "print('Unique glosses:', len(glosses))\n",
    "\n",
    "# https://stackoverflow.com/questions/42320834/sklearn-changing-string-class-label-to-int\n",
    "# Encode the glosses as numerical\n",
    "le = preprocessing.LabelEncoder()\n",
    "fitted = le.fit(glosses)\n",
    "\n",
    "# Store the transformations of the glosses in a dictionary\n",
    "# I.e. string label -> int label\n",
    "keys = le.classes_\n",
    "values = le.transform(le.classes_)\n",
    "dictionary = dict(zip(keys, values))\n",
    "store_dict(PATHS['label_encoder'].format(top_suffix), dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1014642d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdataset \t\t\tdata shape \t\t label shape\n",
      "Train                    \t(180432, 10, 174)   \t\t(180432,)\n",
      "Train (no mirror)        \t(90216, 10, 174)    \t\t(90216,)\n",
      "Validation               \t(10500, 10, 174)    \t\t(10500,)\n",
      "Test                     \t(9719, 10, 174)     \t\t(9719,)\n"
     ]
    }
   ],
   "source": [
    "# New num. annotations for train, test set (filtering by present ratio, removing NaNs)\n",
    "def prep_X(X):\n",
    "    X = X.reshape(*X.shape[:-2], -1).astype(np.float32)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X\n",
    "\n",
    "datasets = ['Train', 'Train (no mirror)', 'Validation', 'Test']\n",
    "total_instances = 0\n",
    "\n",
    "print('\\tdataset \\t\\t\\tdata shape \\t\\t label shape')\n",
    "for i in range(len(X)):\n",
    "    # Convert all to numpy\n",
    "    X[i] = np.array(X[i])\n",
    "    y[i] = np.array(y[i])\n",
    "    # Transform the gloss labels (e.g. 'GEBAREN-A') to numerical (e.g. 123)\n",
    "    y[i] = fitted.transform(y[i])\n",
    "    # Make sure data has shape (num_data_points, num_frames, num_features)\n",
    "    if len(X[i].shape) > 3:\n",
    "        X[i] = prep_X(X[i])\n",
    "    # Store as smaller representation to save space, also required later when training \n",
    "    # Because otherwise it takes too much memory \n",
    "    X[i] = X[i].astype(np.float16) \n",
    "    print('{:<25}\\t{:<20}\\t\\t{}'.format(datasets[i], str(X[i].shape), y[i].shape))\n",
    "    if datasets[i] != 'Train':\n",
    "        total_instances += X[i].shape[0]\n",
    "    \n",
    "X_train, X_train_no_mirr, X_val, X_test = X\n",
    "y_train, y_train_no_mirr, y_val, y_test = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42e823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of annotations: 110435\n",
      "Train (no mirror): 82%\tValidation: 10%\tTest: 9%\t"
     ]
    }
   ],
   "source": [
    "print('Total number of annotations:', total_instances)\n",
    "for i in range(1, len(X)):\n",
    "    print('{}: {}%'.format(datasets[i], round(X[i].shape[0]/total_instances*100)), end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4629a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10295, 10, 174) (9585, 10, 174)\n"
     ]
    }
   ],
   "source": [
    "# We want to only keep validation and test examples\n",
    "# Which have labels that have been seen during training (i.e. are in the train set)\n",
    "in_train_labels = np.unique(y_train)\n",
    "val_ind = np.where(np.isin(y_val, in_train_labels))[0]\n",
    "test_ind = np.where(np.isin(y_test, in_train_labels))[0]\n",
    "X_val, y_val = X_val[val_ind], y_val[val_ind]\n",
    "X_test, y_test = X_test[test_ind], y_test[test_ind]\n",
    "print(X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab7eb0",
   "metadata": {},
   "source": [
    "# Store datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63fded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets and labels to their respective numpy files\n",
    "root = PATHS['data_linguistic'] if only_features else PATHS['data_only_lmrks']\n",
    "# PATHS format: X or y, type of data (train, val, test), features or not, top X signs \n",
    "np.save(root.format('X', 'train', features, top_suffix), X_train)\n",
    "np.save(root.format('y', 'train', features, top_suffix), y_train)\n",
    "\n",
    "np.save(root.format('X', 'train_no_mirror', features, top_suffix), X_train_no_mirr)\n",
    "np.save(root.format('y', 'train_no_mirror', features, top_suffix), y_train_no_mirr)\n",
    "\n",
    "np.save(root.format('X', 'val', features, top_suffix), X_val)\n",
    "np.save(root.format('y', 'val', features, top_suffix), y_val)\n",
    "\n",
    "np.save(root.format('X', 'test', features, top_suffix), X_test)\n",
    "np.save(root.format('y', 'test', features, top_suffix), y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed504aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(836, 182, 1252, 828, 1671)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = load_dict(PATHS['label_encoder'].format(top_suffix))\n",
    "labels['GEBOREN-A'], labels['AMSTERDAM'], labels['KLAAR-A'], labels['GEBAREN-A'], labels['NU-A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f33524",
   "metadata": {},
   "source": [
    "# Finding highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28d5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.64 s\n",
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(991431, 174)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Convert the variable length train data to numpy\n",
    "X_train = np.array(X_train_var_len)\n",
    "y_train = np.array(y_train_var_len)\n",
    "\n",
    "# The above code gives us unevenly sized annotations, we can't just reshape\n",
    "# So to add each frame as a row, we do it in a loop\n",
    "x = []\n",
    "for data_point in X_train:\n",
    "    for frame in data_point:\n",
    "        x.append(frame)\n",
    "        \n",
    "# We mask any NaN values so we can ignore them during later computations\n",
    "x = np.array(x)\n",
    "x = ma.masked_invalid(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f863a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrs being created...\n",
      "(991431, 174)\n",
      "172\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:28: UserWarning: Warning: converting a masked element to nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "CPU times: total: 3h 29min 12s\n",
      "Wall time: 1h 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NOTE: this takes very long to compute (45-60 minutes on the tested hardware)\n",
    "# This is why we try to avoid recomputing this if possible\n",
    "\n",
    "# We compute the correlations between the values in the train data\n",
    "corrs_path = PATHS['masked_corrs'].format(features)\n",
    "\n",
    "# If we already computed the correlations, reuse them\n",
    "if os.path.exists(corrs_path):\n",
    "    print('Corrs being loaded...')\n",
    "    corrs = np.load(corrs_path)\n",
    "    cols = range(x.shape[-1])\n",
    "    # We need to convert to a masked array (can't save it like that)\n",
    "    if 'masked' in corrs_path:\n",
    "        corrs = ma.masked_invalid(corrs)\n",
    "    corrs = list(corrs)\n",
    "else: # Else, compute the correlations (this takes a while)\n",
    "    print('Corrs being created...')\n",
    "    cols = range(x.shape[-1])\n",
    "    print(x.shape)\n",
    "\n",
    "    corrs = []\n",
    "    for col_1 in range(x.shape[-1]-1):\n",
    "        for col_2 in range(col_1+1, x.shape[-1]):\n",
    "            print(col_1, end = '\\r')\n",
    "            corr = ma.corrcoef(x[:,col_1], x[:, col_2])[0][1]\n",
    "            corrs.append((corr, col_1, col_2))\n",
    "    corrs_np = np.array(corrs)\n",
    "    np.save(corrs_path, corrs_np)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecd196dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correlations:\t\t15051\n",
      "Num. of correlations >= 0.9:\t156\n",
      "\n",
      "To be removed: [128, 130, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 145, 23, 24, 147, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 169, 171, 170, 172, 58, 60, 62, 64, 78, 81, 82, 83, 84, 85, 93, 95, 98, 99, 100, 101, 102, 103, 107, 110, 111, 149, 113, 114, 115, 116, 117, 119, 151, 121, 122, 124, 126]\n",
      "\n",
      "Number of features we remove: 75\n",
      "Number of features remaining: 99\n",
      "\n",
      "\n",
      "High correlation tuples:\n",
      "\n",
      "[(0.9918264022853222, 82, 84), (0.989294890155836, 80, 82), (0.9888438684556139, 11, 13), (0.9844714877917816, 30, 39), (0.9832199152699825, 167, 169), (0.9825041023592658, 32, 41), (0.9820304852600885, 81, 83), (0.9815306782122758, 2, 24), (0.981367857250015, 14, 16), (0.9809075593669315, 98, 100), (0.9795712713413487, 83, 85), (0.9786877836153659, 169, 171), (0.9780112144012227, 4, 30), (0.9780108874448653, 0, 11), (0.9769964252230071, 12, 14), (0.9762570250024152, 101, 103), (0.9730806644789081, 24, 26), (0.9725123590536708, 6, 32), (0.9707694317332057, 32, 39), (0.9700971438677655, 30, 35), (0.9685661201142776, 170, 172), (0.9682175547790663, 0, 13), (0.9678623452069943, 34, 41), (0.9675114856930441, 136, 145), (0.967391250514451, 119, 128), (0.966550853528936, 99, 101), (0.9656333708875735, 27, 29), (0.965503214341649, 49, 58), (0.9647552314452956, 8, 34), (0.9644021965411672, 80, 84), (0.9625585354179215, 13, 15), (0.9625345688260022, 121, 128), (0.9624650986599587, 111, 113), (0.9623302647668641, 117, 122), (0.9615967105977091, 4, 39), (0.9615937865149788, 35, 39), (0.9614166912970901, 89, 111), (0.9601654494292872, 140, 149), (0.9584764309233117, 168, 170), (0.9582643683618526, 119, 126), (0.9577556133219898, 4, 6), (0.9575627578277778, 30, 32), (0.9564046261246167, 53, 62), (0.9561037265403697, 142, 151), (0.9560029907332646, 138, 147), (0.9559572912418111, 91, 117), (0.9548230153242657, 51, 60), (0.9537128706694701, 2, 26), (0.9518747851740463, 55, 64), (0.9511296568883227, 39, 41), (0.9506261932677006, 38, 43), (0.950249069736166, 6, 41), (0.9502441725736456, 6, 8), (0.9499007954648355, 6, 39), (0.9498990057539377, 24, 35), (0.9497998916056816, 4, 35), (0.949513791102922, 79, 81), (0.9494577855448213, 9, 15), (0.9487017872458524, 32, 34), (0.9484460537886299, 114, 116), (0.94720067316344, 9, 13), (0.9450400596976782, 81, 85), (0.945004285433357, 26, 28), (0.9449620184570794, 93, 119), (0.9447019469997389, 8, 41), (0.943684098480023, 11, 15), (0.943088524976568, 9, 11), (0.942254340953804, 36, 38), (0.9421023435566034, 117, 126), (0.9403410630865706, 87, 98), (0.9396490367393778, 35, 37), (0.9390216615213508, 93, 95), (0.9388013134184185, 29, 38), (0.9386640807199101, 6, 30), (0.9370365083220866, 119, 121), (0.9355917340306008, 25, 27), (0.9355758483446885, 95, 121), (0.9338686633236296, 4, 32), (0.9337270380476154, 12, 16), (0.9334706134990596, 8, 32), (0.9330580186095788, 21, 23), (0.9325100800219631, 100, 102), (0.9324734459140199, 0, 15), (0.932225496273356, 30, 41), (0.9314239857602503, 2, 35), (0.9307232031737475, 76, 78), (0.9304879475728333, 117, 119), (0.9299856756691341, 91, 93), (0.9295090984231233, 19, 32), (0.9289968767094232, 29, 36), (0.9289881621135044, 0, 9), (0.9289358159904514, 122, 126), (0.9279602479374464, 30, 37), (0.9266281644990699, 167, 171), (0.9264549068626162, 126, 128), (0.9260936119188846, 19, 30), (0.9259382843031697, 18, 20), (0.9259244880793989, 6, 19), (0.9253243401602227, 87, 100), (0.9250642929624644, 1, 18), (0.9241842314152183, 37, 42), (0.9234631808212062, 21, 34), (0.9234527057129532, 17, 24), (0.922511931929555, 89, 113), (0.9215166720233843, 99, 103), (0.9203618246652312, 2, 17), (0.9202240647814305, 37, 39), (0.920213871497967, 8, 21), (0.9201090446392961, 91, 122), (0.919764789850369, 4, 37), (0.9197249733785618, 108, 110), (0.9195219225595647, 4, 19), (0.919513886611946, 32, 42), (0.9190448601532599, 19, 21), (0.9189074141353585, 13, 20), (0.918436722909238, 24, 28), (0.9178293862184406, 79, 83), (0.9165172410462435, 26, 35), (0.9157553125004679, 42, 44), (0.9146962866777285, 19, 39), (0.9133869932096089, 10, 12), (0.9125647113777294, 19, 42), (0.9120064761801141, 6, 34), (0.91151523659269, 113, 115), (0.9112950064722686, 93, 126), (0.9109520647918256, 95, 128), (0.9107926600620206, 105, 107), (0.9104142568611471, 2, 28), (0.91035115734512, 29, 31), (0.9101165455080252, 111, 122), (0.9099725675240581, 16, 31), (0.9097836467883267, 6, 42), (0.9091787299467894, 24, 30), (0.9085669067331793, 96, 102), (0.9080732829747803, 98, 102), (0.9068784554315351, 125, 130), (0.9068297796469219, 4, 41), (0.9065830584412609, 2, 4), (0.9061084552570485, 168, 172), (0.9057055911677663, 34, 44), (0.9054030419876746, 39, 42), (0.9053723833860252, 11, 20), (0.9052275023512079, 19, 37), (0.9043058180047822, 112, 114), (0.9021747672374404, 97, 99), (0.9020242598533456, 13, 16), (0.9015337447891784, 95, 119), (0.9015221786040867, 122, 124), (0.9014573469241197, 91, 124), (0.9012307391302476, 21, 32), (0.9006243437010524, 93, 128), (0.9005632520803427, 21, 44), (0.9005245698280057, 13, 31), (0.900232820676051, 41, 42), (0.9001045777765913, 32, 35), (0.9000817438282763, 117, 124)]\n"
     ]
    }
   ],
   "source": [
    "# Continuation of the block above this one\n",
    "print('Total correlations:\\t\\t{}'.format(len(corrs)))\n",
    "num_high_corr = 0\n",
    "high_corrs = []\n",
    "removable = []\n",
    "corr_thresh = 0.9\n",
    "for corr in corrs:\n",
    "    if abs(corr[0]) >= corr_thresh:\n",
    "        corr = [corr[0], int(corr[1]), int(corr[2])]\n",
    "        high_corrs.append(tuple(corr))\n",
    "        # Remove the first feature of the high corr. pair\n",
    "        removable.append(sorted(corr[1:])[-1])\n",
    "        num_high_corr += 1\n",
    "print('Num. of correlations >= {}:\\t{}'.format(corr_thresh, num_high_corr))\n",
    "to_remove = list(set(removable))\n",
    "\n",
    "print('\\nTo be removed:', to_remove)\n",
    "remain = set(cols) - set(to_remove)\n",
    "\n",
    "print('\\nNumber of features we remove:', len(to_remove))\n",
    "print('Number of features remaining:', len(list(remain)))\n",
    "\n",
    "print('\\n\\nHigh correlation tuples:\\n')\n",
    "print(sorted(high_corrs, key = lambda x: x[0], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e77953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15051\n",
      "to_remove: [1, 2, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 53, 54, 58, 60, 62, 64, 78, 79, 81, 82, 83, 84, 85, 91, 93, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 140, 141, 145, 147, 149, 151, 167, 168, 169, 170, 171, 172, 173]\n",
      "Number of features we remove: 100\n",
      "Number of features remaining: 74\n"
     ]
    }
   ],
   "source": [
    "# Continuation of the block above this one\n",
    "print(len(corrs))\n",
    "num_high_corr = 0\n",
    "high_corrs = []\n",
    "removable = []\n",
    "for corr in corrs:\n",
    "    if abs(corr[0]) >= 0.80:\n",
    "        high_corrs.append(corr)\n",
    "        removable.append(corr[-1])\n",
    "        num_high_corr += 1\n",
    "# print(num_high_corr)\n",
    "# print(list(set(removable)))\n",
    "# print(sorted(high_corrs, key = lambda x: x[0], reverse = True))\n",
    "\n",
    "to_remove = np.array(list(set(removable))).astype(np.int64)\n",
    "remain = set(cols) - set(to_remove)\n",
    "print('to_remove:', list(to_remove))\n",
    "print('Number of features we remove:', len(to_remove))\n",
    "print('Number of features remaining:', len(list(remain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c4a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "Remain length: 74\n",
      "Remaining of left hand:\n",
      "[0, 3, 5, 7, 10, 25, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 59, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 86] (35 features)\n",
      "Remaining of right hand:\n",
      "[0, 1, 2, 3, 5, 7, 10, 22, 25, 33, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 59, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] (39 features)\n",
      "Extra features right hand: [1, 2, 22, 33, 78, 79]\n",
      "Extra features left hand: [80, 86]\n",
      "Same for both hands: [0, 3, 5, 7, 10, 25, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 59, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]\n"
     ]
    }
   ],
   "source": [
    "# Analyze which features remain\n",
    "def feature_selection_analysis(num_features, to_remove):\n",
    "    half_features = round(num_features/2) - 1\n",
    "    print(half_features)\n",
    "    rem = set(range(x.shape[-1])) - set(to_remove)\n",
    "    print('Remain length:', len(rem))\n",
    "    rem_first = [r for r in sorted(rem) if r <= half_features]\n",
    "    rem_second = [r-(half_features+1) for r in sorted(rem) if r > half_features]\n",
    "    print('Remaining of left hand:')\n",
    "    print(rem_first, '({} features)'.format(len(rem_first)))\n",
    "    print('Remaining of right hand:')\n",
    "    print(rem_second, '({} features)'.format(len(rem_second)))\n",
    "\n",
    "    print('Extra features right hand:', sorted(set(rem_second) - set(rem_first)))\n",
    "    print('Extra features left hand:', sorted(set(rem_first) - set(rem_second)))\n",
    "    \n",
    "    print('Same for both hands:', sorted(set(rem_second) & set(rem_first)))\n",
    "# We use cols to get the number of features\n",
    "feature_selection_analysis(len(cols), to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb3dcd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 9527, 0.1: 1395, -0.1: 817, 0.7: 533, 0.6: 508, 0.2: 420, 0.8: 392, 0.5: 260, 0.9: 240, -0.2: 206, 0.3: 192, 0.4: 85, 1.0: 53, -0.3: 46, -0.4: 24, -0.5: 3, -0.6: 3, -0.7: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\thesis\\lib\\site-packages\\numpy\\ma\\core.py:2826: UserWarning: Warning: converting a masked element to nan.\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# We round the values in the correlations, then remove masked values because we can't count them\n",
    "# Then we print a counter of the (rounded) correlations\n",
    "corrs_round = np.ma.array([np.round(c[0], 1) for c in corrs])\n",
    "corrs_round_not_masked = corrs_round[~corrs_round.mask]\n",
    "print(Counter(corrs_round_not_masked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "54d668a7a2b723eaa45db485aeb9405c48b3714a12ae318aa481966bf3ee5079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
